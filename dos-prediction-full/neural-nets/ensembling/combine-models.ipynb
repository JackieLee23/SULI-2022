{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03e96d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import os\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "root = \"/project/wyin/jlee/ml-project/dos-prediction\"\n",
    "util_loc = os.path.join(root, \"utils\")\n",
    "data_loc = os.path.join(root, \"data\")\n",
    "sys.path.append(util_loc)\n",
    "from utilities import LitNeuralNet, DosDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7271ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "same_models = []\n",
    "for i in range(24):\n",
    "    log_path = \"logs/long-run-24\"\n",
    "    log_dir = os.listdir(log_path)[i]\n",
    "    check_dir = os.path.join(log_path, log_dir, \"version_0/checkpoints\")\n",
    "    check_file = os.listdir(check_dir)[0]\n",
    "    check_path = os.path.join(check_dir, check_file)\n",
    "    model = LitNeuralNet.load_from_checkpoint(checkpoint_path=check_path)\n",
    "    model_name = log_dir.rpartition(']')[0] + ']'\n",
    "    \n",
    "    if model_name != \"[3, 256, 512, 768, 1024, 301]\":\n",
    "        models.append((model_name, model))\n",
    "        \n",
    "    else:\n",
    "        same_models.append(model)\n",
    "    \n",
    "    \n",
    "for i in range(10):\n",
    "    log_path = \"logs/long-run-10\"\n",
    "    log_dir = os.listdir(log_path)[i]\n",
    "    check_dir = os.path.join(log_path, log_dir, \"version_1/checkpoints\")\n",
    "    check_file = os.listdir(check_dir)[0]\n",
    "    check_path = os.path.join(check_dir, check_file)\n",
    "    model = LitNeuralNet.load_from_checkpoint(checkpoint_path=check_path)\n",
    "    model_name = log_dir.rpartition(']')[0] + ']'\n",
    "    models.append((model_name, model))\n",
    "    \n",
    "print(len(models))\n",
    "print(len(same_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0bfbb428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13265, 3])\n"
     ]
    }
   ],
   "source": [
    "dataset = np.load(f\"{data_loc}/train-set.npz\")\n",
    "train_params = dataset['params']\n",
    "\n",
    "dataset = np.load(f\"{data_loc}/val-set.npz\")\n",
    "val_params_arr = dataset['params']\n",
    "val_dos_arr = dataset['dos']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_params)\n",
    "val_params_arr = scaler.transform(val_params_arr)\n",
    "\n",
    "val_params_arr = torch.from_numpy(val_params_arr).float()\n",
    "val_dos_arr = torch.from_numpy(val_dos_arr).float()\n",
    "print(params_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0036278b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 256, 512, 768, 1024, 301] MSE: 5.1276829253765754e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e76136e1d3a40b69da9b48ab68d8f60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        val_loss           5.127681561134523e-06\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "def mse(model):\n",
    "    predicted = model.forward_prop(val_params_arr)\n",
    "    return F.mse_loss(predicted, val_dos_arr).item()\n",
    "\n",
    "\n",
    "#Test random model for sanity check\n",
    "model_name, model = models[20]\n",
    "print(f\"{model_name} MSE: {mse(model)}\")\n",
    "\n",
    "batch_size = 1024\n",
    "dos_data = DosDataModule(data_loc, batch_size)\n",
    "dos_data.setup()\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience = 100)\n",
    "trainer = pl.Trainer()\n",
    "end_res = trainer.validate(model, dataloaders = dos_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7e8acf02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 170, 340, 510, 680, 850, 1020, 301]: 4.5635088099516e-06\n",
      "[3, 204, 408, 612, 816, 1020, 301]: 5.042199518356938e-06\n",
      "[3, 512, 512, 512, 512, 512, 512, 512, 301]: 5.054268058302114e-06\n",
      "[3, 146, 292, 438, 584, 730, 876, 1022, 301]: 5.08028506374103e-06\n",
      "[3, 256, 512, 768, 1024, 301]: 5.1276829253765754e-06\n",
      "[3, 128, 256, 384, 512, 640, 768, 896, 1024, 301]: 5.565386345551815e-06\n",
      "[3, 512, 512, 512, 512, 512, 512, 301]: 5.6197241065092385e-06\n",
      "[3, 128, 256, 512, 1024, 301]: 5.810610673506744e-06\n",
      "[3, 64, 128, 256, 512, 1024, 301]: 6.176588613016065e-06\n",
      "[3, 32, 64, 128, 256, 512, 1024, 301]: 7.037332579784561e-06\n",
      "[3, 512, 512, 512, 512, 512, 301]: 7.051531156321289e-06\n",
      "[3, 256, 512, 1024, 301]: 7.409566023852676e-06\n",
      "[3, 73, 146, 219, 292, 365, 438, 511, 301]: 7.5117882261110935e-06\n",
      "[3, 85, 170, 255, 340, 425, 510, 301]: 7.6142523539601825e-06\n",
      "[3, 512, 512, 512, 512, 301]: 7.843245839467272e-06\n",
      "[3, 64, 128, 192, 256, 320, 384, 448, 512, 301]: 7.86020973464474e-06\n",
      "[3, 102, 204, 306, 408, 510, 301]: 8.230287676269654e-06\n",
      "[3, 16, 32, 64, 128, 256, 512, 1024, 301]: 9.20345883059781e-06\n",
      "[3, 128, 256, 384, 512, 301]: 9.561171282257419e-06\n",
      "[3, 256, 256, 256, 256, 256, 256, 256, 301]: 1.0574057341727894e-05\n",
      "[3, 256, 256, 256, 256, 256, 256, 256, 256, 301]: 1.0767806088551879e-05\n",
      "[3, 256, 256, 256, 256, 256, 301]: 1.1840107617899776e-05\n",
      "[3, 64, 128, 256, 512, 301]: 1.2228482773934957e-05\n",
      "[3, 32, 64, 128, 256, 512, 301]: 1.2516793503891677e-05\n"
     ]
    }
   ],
   "source": [
    "sorted_models = sorted(models, key = lambda x : mse(x[1]))\n",
    "for model in sorted_models:\n",
    "    print(f'{model[0]}: {mse(model[1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bd3f3902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5635088099516e-06\n",
      "3.7378347315097926e-06\n",
      "3.4210693229397293e-06\n",
      "3.3009262097039027e-06\n",
      "3.1860188300925074e-06\n",
      "3.1874094474915182e-06\n",
      "3.1775641673448263e-06\n",
      "3.163130941175041e-06\n",
      "3.177038706780877e-06\n",
      "3.2260377338388935e-06\n",
      "3.2719708542572334e-06\n",
      "3.295528586022556e-06\n",
      "3.3352878290315857e-06\n",
      "3.383306875548442e-06\n",
      "3.4321012662985595e-06\n",
      "3.4652941849344643e-06\n",
      "3.5068778743152507e-06\n",
      "3.5554378428059863e-06\n",
      "3.6119131436862517e-06\n",
      "3.687421440190519e-06\n",
      "3.763792847166769e-06\n",
      "3.8472426240332425e-06\n",
      "3.917740741599118e-06\n",
      "3.9872443267086055e-06\n"
     ]
    }
   ],
   "source": [
    "#Ensemble with different architectures\n",
    "ensemble_res = []\n",
    "def ensemble(ens_size):\n",
    "    total_pred = torch.zeros([13265, 301])\n",
    "    total_mse = 0\n",
    "    for model_name, model in sorted_models[:ens_size]:\n",
    "        predicted = model.forward_prop(params_arr)\n",
    "        error = mse(model)\n",
    "        total_mse += error\n",
    "        total_pred = torch.add(total_pred, predicted)\n",
    "\n",
    "    avg_pred = total_pred / ens_size\n",
    "    ens_mse = F.mse_loss(avg_pred, dos_arr).item()\n",
    "    print(ens_mse)\n",
    "    return ens_mse\n",
    "    \n",
    "for i in range(1, 25):\n",
    "    ensemble_res.append(ensemble(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "490ea367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.163130941175041e-06\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print(min(ensemble_res))\n",
    "print(ensemble_res.index(min(ensemble_res)) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1f9667c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.932330284646014e-06\n",
      "3.8558978303626645e-06\n",
      "3.519755637171329e-06\n",
      "3.3416256428608904e-06\n",
      "3.2547968658036552e-06\n",
      "3.1889212550595403e-06\n",
      "3.1620181744074216e-06\n",
      "3.139731461487827e-06\n",
      "3.136958184768446e-06\n",
      "3.168372813888709e-06\n"
     ]
    }
   ],
   "source": [
    "#Ensemble with best architecture\n",
    "ensemble_res = []\n",
    "sorted_same_models = sorted(same_models, key = lambda x : mse(x))\n",
    "def ensemble(ens_size):\n",
    "    total_pred = torch.zeros([13265, 301])\n",
    "    total_mse = 0\n",
    "    for model in sorted_same_models[:ens_size]:\n",
    "        predicted = model.forward_prop(params_arr)\n",
    "        error = mse(model)\n",
    "        total_mse += error\n",
    "        total_pred = torch.add(total_pred, predicted)\n",
    "    \n",
    "    avg_pred = total_pred / ens_size\n",
    "    ens_mse = F.mse_loss(avg_pred, dos_arr).item()\n",
    "    print(ens_mse)\n",
    "    return ens_mse\n",
    "\n",
    "for i in range(1, 11):\n",
    "    ensemble_res.append(ensemble(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "875bc750",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layer_sizes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m dos_data \u001b[38;5;241m=\u001b[39m DosDataModule(data_loc, batch_size)\n\u001b[0;32m----> 2\u001b[0m logger \u001b[38;5;241m=\u001b[39m TensorBoardLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogs/long-run-24-test\u001b[39m\u001b[38;5;124m'\u001b[39m, name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_sizes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlearning_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mschedule_factor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step_num \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m:\n\u001b[1;32m      4\u001b[0m     logger \u001b[38;5;241m=\u001b[39m TensorBoardLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogs/long-run-24-test\u001b[39m\u001b[38;5;124m'\u001b[39m, name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_sizes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlearning_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mschedule_factor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep_num \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'layer_sizes' is not defined"
     ]
    }
   ],
   "source": [
    "#Use 10 best models and average"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
